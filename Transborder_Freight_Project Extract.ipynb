{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1abbec-f9de-4c65-bfdf-8962fd4447fe",
   "metadata": {},
   "source": [
    "# U.S. Transborder Freight Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d60e2-ab0d-47c6-ba3b-c07ad6e9e773",
   "metadata": {},
   "source": [
    "### This Notebook shows the code used to extract the CSVs from its zipped folder and to combine them into one CSV for analysis in Power BI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96986d62-957e-45a6-9f9d-cf51ef9ebcd4",
   "metadata": {},
   "source": [
    "### File Extraction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2cc1f62-279c-45ce-b4fd-137ba3b8fd18",
   "metadata": {},
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def extract_all_zip_recursive(zip_path, extract_to):\n",
    "    def _extract(zip_path, extract_dir):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "       \n",
    "\n",
    "        for root, _, files in os.walk(extract_dir):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                if zipfile.is_zipfile(full_path):\n",
    "                    new_extract_dir = os.path.splitext(full_path)[0]\n",
    "                    os.makedirs(new_extract_dir, exist_ok=True)\n",
    "                    _extract(full_path, new_extract_dir)\n",
    "                    \n",
    "\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    _extract(zip_path, extract_to)\n",
    "\n",
    "\n",
    "# Changing the location of the files to be unzipped\n",
    "zip_file = 'data.zip'\n",
    "output_dir = 'unzipped_project_data'\n",
    "\n",
    "extract_all_zip_recursive(zip_file, output_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6326f75d-3cad-494b-923f-07c8751a3dfd",
   "metadata": {},
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def flatten_csv_files(data_folder):\n",
    "    # Walk through all subdirectories in the data folder\n",
    "    for dirpath, dirnames, filenames in os.walk(data_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                src_path = os.path.join(dirpath, filename)\n",
    "                dst_path = os.path.join(data_folder, filename)\n",
    "\n",
    "                # Rename file if there's a conflict\n",
    "                if os.path.exists(dst_path):\n",
    "                    base, ext = os.path.splitext(filename)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(dst_path):\n",
    "                        dst_path = os.path.join(data_folder, f\"{base}_{counter}{ext}\")\n",
    "                        counter += 1\n",
    "\n",
    "                shutil.move(src_path, dst_path)\n",
    "\n",
    "    # Remove empty subfolders\n",
    "    for dirpath, dirnames, filenames in os.walk(data_folder, topdown=False):\n",
    "        if dirpath != data_folder and not dirnames and not filenames:\n",
    "            os.rmdir(dirpath)\n",
    "\n",
    "# Path to the unzipped_project_data folder\n",
    "data_folder = r'unzipped_project_data'\n",
    "\n",
    "flatten_csv_files(data_folder)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56fa1f11-ed4d-4694-8b58-f6fbe9364122",
   "metadata": {},
   "source": [
    "def load_multiple_csvs_from_zip(data):\n",
    "    \"\"\"\n",
    "    Loads all CSV files from a ZIP archive into a dictionary of pandas DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "\n",
    "    with zipfile.ZipFile(data.zip, 'r') as zip_ref:\n",
    "        # Loop through all files in the ZIP\n",
    "        for filename in zip_ref.namelist():\n",
    "            # Only process .csv files\n",
    "            if filename.endswith('.csv'):\n",
    "                with zip_ref.open(data) as file:\n",
    "                    df = pd.read_csv(data)\n",
    "                    dataframes[data] = df\n",
    "                    print(f\"Loaded {data}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb0519-6001-4723-98f0-6138859b0988",
   "metadata": {},
   "source": [
    "### Combining all CSVs into a single CSV"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6342ef8-772d-48db-a30e-e2127871052d",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "def append_csvs_by_filename_pattern(folder_path):\n",
    "    all_dataframes = []\n",
    "\n",
    "    # Looping through all files in folder and subfolders\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            # Checking for .csv and starts with dot\n",
    "            if filename.lower().endswith('.csv') and filename.startswith('dot'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, dtype={4: str, 11: str})\n",
    "\n",
    "                    # Tracking file source (optional)\n",
    "                    df['source_file'] = filename\n",
    "\n",
    "                    all_dataframes.append(df)\n",
    "                    print(f\"✔ Loaded: {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Error reading {filename}: {e}\")\n",
    "\n",
    "    # Combining all loaded DataFrames\n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No matching CSV files found.\")\n",
    "        return None\n",
    "\n",
    "# Path to 'data' folder\n",
    "data_folder = r'unzipped_project_data'\n",
    "\n",
    "# Run function\n",
    "result_df = append_csvs_by_filename_pattern(data_folder)\n",
    "\n",
    "# Saving result as all concat\n",
    "if result_df is not None:\n",
    "    output_file = os.path.join(data_folder, 'all_concat.csv')\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n Combined CSV saved to:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ab661-c3d5-4fa2-8516-13f9238f7eb3",
   "metadata": {},
   "source": [
    "### Loading dataset to check columns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f008223-f4f9-40df-b292-4271f4fcd1be",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "freight_data = pd.read_csv('unzipped_project_data/all_concat.csv', low_memory=False))\n",
    "freight_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36414b49-d944-4745-b4c8-e55cb0ba1f0e",
   "metadata": {},
   "source": [
    "#### The dataset was then loaded into Power BI for data cleaning, processing and answering business questions via visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997cdb0-b61f-48c2-abdb-0b2e661893a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
